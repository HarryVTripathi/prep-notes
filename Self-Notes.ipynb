{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c55e6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 {\n",
       "  border-radius: 10px;\n",
       "  padding: 10px;\n",
       "  margin: 10px;\n",
       "  font-family: cursive;\n",
       "  color: white;\n",
       "  position: static;\n",
       "  background: rgb(0,206,244);\n",
       "  background: linear-gradient(180deg, rgba(0,206,244,1) 0%, rgba(169,238,251,1) 100%);\n",
       "}\n",
       "\n",
       "div#notebook {\n",
       "  color: black;\n",
       "  background-color: rgb(0, 0, 0);\n",
       "}\n",
       "\n",
       "div#notebook-container {\n",
       "  color: black;\n",
       "  background-color: rgb(32, 35, 36);\n",
       "}\n",
       "\n",
       "div.text_cell_render {\n",
       "  word-spacing: 120%;\n",
       "  font-family: cursive;\n",
       "  font-size: 120%;\n",
       "  color: rgb(249, 250, 250);\n",
       "  line-height: 220%;\n",
       "}\n",
       "\n",
       "div.output_subarea.output_text.output_result {\n",
       "  color: aliceblue;\n",
       "  background: rgb(196, 209, 212);\n",
       "  background: linear-gradient(180deg,rgb(196, 209, 212, 1) 0%, rgb(255, 255, 255) 100%);\n",
       "} \n",
       "\n",
       "div.CodeMirror-lines {\n",
       "  background-color: rgb(196, 209, 212);\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57267f4",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "The amount of variance in Response, explained by the variance in Predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd30d48",
   "metadata": {},
   "source": [
    "Normalization: Reduces interpretability. But keeps gradients smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737645d0",
   "metadata": {},
   "source": [
    "#### Pressumptions:\n",
    "\n",
    " - Response (Target) variable should follow normal distribution.\n",
    " \n",
    "The optimal weights are the ones which maximize likelihood. The probabilities are computed assuming normal distribution. Likelihood of y | X, is computed using PDF of normal distribution.\n",
    " \n",
    " - No outliers.\n",
    "\n",
    "Can seriously affect the estimated fit line.\n",
    "\n",
    " - Predictors should have a linear relationship with target variable.\n",
    " \n",
    "If there are too many outliers, making the distribution skewed, we can log transform the target variable, use ln(response_variable), instead of response_variable.\n",
    " \n",
    " - No multicollinearity (predictors should be independent of each other).\n",
    " \n",
    "Because while calculating partial derivatives, we assume that a predictor is not a function of another. Also, having a linear dependence would mean a redundant feature.\n",
    "\n",
    "Can use Variance Inflation Factor (VIF) to assess multi-collinearity.\n",
    " \n",
    " - No homoescadasticity.\n",
    "\n",
    "#### Loss:\n",
    "\n",
    "Squared loss: $ \\frac{1}{n} \\sum \\limits_{i=1}^{n} (y_i - \\hat y_i)^2 $\n",
    "\n",
    "#### Metric:\n",
    "\n",
    " - Coefficient of determination: R squared\n",
    " \n",
    "compares the goodness of the model to a mean model\n",
    "\n",
    "$ R^2 = 1 - \\frac{RSS}{TSS} $\n",
    "\n",
    "where Residual Sum of Squares, RSS = $ \\sum \\limits_{i=1}^{n} (y_i - \\hat y_i)^2 $\n",
    "\n",
    "and Total Sum of Squares, TSS = $ \\sum \\limits_{i=1}^{n} (y_i - \\bar y)^2 $\n",
    "\n",
    " - Adjusted R squared.\n",
    " \n",
    "Adding additional predictors will always increase the $ R^2 $ metric.\n",
    "\n",
    "$ 1 - \\large \\frac{(n-1)R^2}{n-d-1} $\n",
    "\n",
    "$ \\hspace{5mm} $ where, n &rarr; no of observations (or batch size)\n",
    "\n",
    "$ \\hspace{5mm} $ and d &rarr; no of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2b430",
   "metadata": {},
   "source": [
    "> **Nit**: Don't fall for Dummy variable trap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed8163",
   "metadata": {},
   "source": [
    "#### Residual analysis\n",
    "\n",
    " - Residuals should follow normal correlation.\n",
    " \n",
    " - No auto-correlation.\n",
    "\n",
    "If errors are behiving in a particular fashion.\n",
    "\n",
    " - No heteroscadasticity or homoscadasticity or or homogeneity of variances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17072656",
   "metadata": {},
   "source": [
    "#### Handling categorical features\n",
    "\n",
    " - One hot encoding\n",
    " \n",
    " - Target encoding\n",
    " \n",
    " Replace the categorical value with aggregate of target for that category.\n",
    " \n",
    " > For example, if we have a categorical field `gender` with values (Man, Woman) and we have target variable `weight`, we can replace the categorical value man with median weight of men. <br> <br> If we get a new data point where we do not have either category, let's say, a datapoint with `non-binary` gender, in that case, we replace it with overall aggregate weight (median here). This is what we call heirarchical smoothing.\n",
    " \n",
    " Ordinal variables can be handled in same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a09fb1",
   "metadata": {},
   "source": [
    "#### Interpretability\n",
    "\n",
    " - Feature importance.\n",
    " \n",
    " Most important features - the ones with biggest absolute weights?\n",
    " \n",
    " Not necessarily. Quantities are in different scales (time in seconds, distance in kilometers) and therefore cannot be compared. Standardize the data.\n",
    " \n",
    " - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb7cd8",
   "metadata": {},
   "source": [
    "> Nitpick: Standardization is a must almost always, as it stablizes gradient descent. So whenever we have an optimization based algorithm, it is recommended to standardize the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bab18f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d8f7ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e9d6b97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "657fc334",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
